---
title: "Practial Machine Learning Project"
author: "Tinguaro Barreno"
date: "27 de septiembre de 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library-load, echo=FALSE}
# Library load
library(caret)
library(survival)
library(randomForest)
library(mlbench)
```

## Summary

The goal of this project is to predict the manner in which some participants
did a weight lifting exercise by training and predicting
over the "**Weight Lifting Exercises Dataset**" data.

> *Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).*

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz4trw5SJMb

### Goals

You should create a report describing:

  - How you built your model
  - How you used cross validation
  - What you think the expected out of sample error is
  - And why you made the choices you did.

You will also use your prediction model to predict 20 different test cases.


#### Getting the data

Let's get the data files to the local filesystem.

```{r data-loading}
# Loads (and download) the training data
training_file = "pml-training.csv"

if( ! file.exists( training_file ) )
{
  download.file( "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                 destfile = training_file )
}
training_orig <- read.csv( training_file )

# Loads (and download) the testing data
testing_file = "pml-testing.csv"

if( ! file.exists( testing_file ) )
{
  download.file( "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                 destfile = testing_file )
}
testing_orig <- read.csv( testing_file )
```

#### Data cleaning

In a quick view over the data sets (not shown due it's size),
we could find that almost half of the variables doesn't
have any data, so I will remove those variables from the data sets.

```{r data-cleaning-1}
training_selected <- training_orig[ , colSums(is.na(training_orig)) == 0 ]
testing_selected  <- testing_orig[ , colSums(is.na(testing_orig)) == 0 ]
```

I will also remove the time stamps, windows, index and username columns because they're
not relevant for the model training and prediction.

```{r data-cleaning-2}
training_selected <- subset(training_selected, 
                            select = - c(new_window, num_window, X,
                                         user_name,
                                         raw_timestamp_part_1,
                                         raw_timestamp_part_2,
                                         cvtd_timestamp ) )
testing_selected <- subset(testing_selected, 
                            select = - c(new_window, num_window, X,
                                         user_name,
                                         raw_timestamp_part_1,
                                         raw_timestamp_part_2,
                                         cvtd_timestamp ) )
```

Finally, there are some columns in the training dataset with incomplete data
(many observations with empty values): I will drop those columns because they're
not present in the testing dataset.

```{r data-cleaning-3}
# First, count the number of "" values in all colums
empty_values_by_column <- sapply(training_selected, function(x) sum( as.character(x) == "" ))

# Get only column names with 0 emtpy values
good_columns <- training_selected[ ,which(empty_values_by_column == 0) ]

# Filter
training_selected <- subset( training_selected, select=colnames( good_columns ) )
```

Now we have (almost) the same set of variables in the training and the testing dataset.
The only difference is the `classe` variable that only applies to the training dataset
and the `problem_id` in the testing dataset.


#### Data Exploratory Analysis

Let's forget the testing data set for now: it will be use for the "real" prediction.

We have $`r nrow(training_selected)`$ observations in the training dataset. All the
values are numeric (or integer) except the `classe` that is a Factor.


#### Data preparation

We will split the current training dataset in three: 70% of the data for training
15% for cross-validation and 15% for testing.

```{r dataset-split}
# Split point
partition_point <- createDataPartition(training_selected$classe, p=0.7)[[1]]

# Training data set
training_set <- training_selected[ partition_point, ]
rest_set <- training_selected[ -partition_point, ]

# Second split (CV/Testing)
partition_point <- createDataPartition(rest_set$classe, p=0.5)[[1]]

# New datasets
cv_set <- rest_set[ partition_point, ]
testing_set <- rest_set[ -partition_point, ]
```

#### Selecting the model

```{r scope-cleaning, echo=FALSE}
# Just cleaning scope variables (training_orig is 19Mb)
rm( testing_orig, training_orig, partition_point, good_columns,
    training_selected, testing_selected, rest_set, empty_values_by_column )
```

The `caret` packages contains
[more than 200 models](https://topepo.github.io/caret/available-models.html)
for machine learning, and there isn't any *rule of thumb* for
choosing the best model at first glance.

My first training attempt with this project was to train the raw data set
with the *Random Forest* (`rf`) method. It ran out of memory several times even with
a reduced data set (about 2,000 observations) in a desktop computer with 4Gb.
I also tried the *Neural Network* method (`nnet`) with better memory efficiency
but it showed a poor performance (innaccuracy). And finally the 
*Gradient Boosted Machine* (`gbm`) that seemed to me the most balanced method.

Then, I cleaned the data set, and following [an interesting article](https://machinelearningmastery.com/compare-models-and-select-the-best-using-the-caret-r-package/)
about comparing models, I will train three models and compare the results.

```{r gbm-tunning}
# Prepare training scheme
control <- trainControl( method="repeatedcv", number=10, repeats=3, classProbs=TRUE )

# Train the LVQ model
set.seed(7)
time_lvq <- system.time(
  model_lvq <- train(classe ~ .,
                     data = training_set,
                     method = "lvq",
                     trControl = control) )

# train the GBM model
set.seed(7)
time_gbm <- system.time( 
  model_gbm <- train(classe ~ ., 
                     data = training_set, 
                     method = "gbm", 
                     trControl = control, 
                     verbose = FALSE) )

# train the SVM model
set.seed(7)
time_svm <- system.time(
  model_svm <- train(classe ~ .,
                     data = training_set,
                     method = "svmRadial",
                     trControl = control) )
```

There is a noteworthy difference in training times between the models:
for example, the *SVM* model training lasted for more than 2 hours while
the *GBM* ran for just 20 minutes.

```{r time-table}
# Just a elapsed-time and object size table
time_table <- matrix( c(time_lvq[3], object.size(model_lvq),
                        time_gbm[3], object.size(model_gbm),
                        time_svm[3], object.size(model_svm) ),
                      ncol=2, byrow=TRUE )

# Units: time in minutes and sizes in Mb
time_table[,1] <- time_table[,1] / 60
time_table[,2] <- time_table[,2] / (1024 * 1024)

rownames( time_table ) <- c( "LVQ", "GBM", "SVM" )
colnames( time_table ) <- c( "Elapsed time (mins.)", "Object size (Mb)" )

# Show the table
time_table
```

Looking at the model accuracy, the best model is **GBM**. It's accuracy and
Kappa values over the training set are about 95%.

```{r comparison-summary}
# collect resamples
results <- resamples( list( LVQ=model_lvq, GBM=model_gbm, SVM=model_svm ) )

# summarize the distributions
summary(results)
```

```{r comparison-plots}
# Boxplots of results
bwplot(results)
```


Now let's evaluate the prediction accuracy for the testing set.


```{r model-predictions}
# Prediction
prediction_lvq <- predict( model_lvq, testing_set )
prediction_gbm <- predict( model_gbm, testing_set )
prediction_svm <- predict( model_svm, testing_set )

# Confusion matrix
cm_lvq <- confusionMatrix( testing_set$classe, prediction_lvq )
cm_gbm <- confusionMatrix( testing_set$classe, prediction_gbm )
cm_svm <- confusionMatrix( testing_set$classe, prediction_svm )


cm_table <- matrix( c(cm_lvq$overall[["Accuracy"]], cm_lvq$overall[["Kappa"]],
                      cm_gbm$overall[["Accuracy"]], cm_gbm$overall[["Kappa"]],
                      cm_svm$overall[["Accuracy"]], cm_svm$overall[["Kappa"]] ),
                      ncol=2, byrow=TRUE )

rownames( cm_table ) <- c( "LVQ", "GBM", "SVM" )
colnames( cm_table ) <- c( "Accuracy", "Kappa" )

# Show the table
cm_table
```

Again, the best model is the **GBM** with an expected error about 5%.

## Test Prediction

```{r prediction}
# PredicciÃ³n con GBM
# predictedProbs <- predict(model_svm, newdata = testing_set , type = "prob")
predict( model_gbm, testing_selected )
```